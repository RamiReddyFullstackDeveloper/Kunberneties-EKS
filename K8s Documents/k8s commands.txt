#!/bin/bash

#Updating resources
============================================================================================
kubectl set image deployment/frontend www=image:v2               # Rolling update "www" containers of "frontend" deployment, updating the image
kubectl rollout history deployment/frontend                      # Check the history of deployments including the revision
kubectl rollout undo deployment/frontend                         # Rollback to the previous deployment
kubectl rollout undo deployment/frontend --to-revision=2         # Rollback to a specific revision
kubectl rollout status -w deployment/frontend                    # Watch rolling update status of "frontend" deployment until completion
kubectl rollout restart deployment/frontend                      # Rolling restart of the "frontend" deployment


cat pod.json | kubectl replace -f -                              # Replace a pod based on the JSON passed into stdin

# Force replace, delete and then re-create the resource. Will cause a service outage.
kubectl replace --force -f ./pod.json


# Create a service for a replicated nginx, which serves on port 80 and connects to the containers on port 8000
kubectl expose rc nginx --port=80 --target-port=8000

kubectl get svc #to view the servies

# Update a single-container pod's image version (tag) to v4
kubectl get pod mypod -o yaml | sed 's/\(image: myimage\):.*$/\1:v4/' | kubectl replace -f -

kubectl label pods my-pod new-label=awesome                      # Add a Label
kubectl label pods my-pod new-label-                             # Remove a label
kubectl label pods my-pod new-label=new-value --overwrite        # Overwrite an existing value
kubectl annotate pods my-pod icon-url=http://goo.gl/XXBTWq       # Add an annotation
kubectl annotate pods my-pod icon-                               # Remove annotation
kubectl autoscale deployment foo --min=2 --max=10                # Auto scale a deployment "foo"


#Editing resources 
=================================================

$ kubectl edit svc/docker-registry  

#Edit the pod
kubectl edit pod <name> #editing the running command, we can make changes for all there are limitaion
or
vim <name-of-the-pod>.yaml  #declarative aproach after made changes run $kubectl apply - <name-of-the-pod.yaml>
#dscribe the pod
$kubectl describe pod/<name>
#dscribe the pod with less command
$kubectl describe pod/<name> | less

#Scaling resources 
=======================
kubectl scale --replicas=3 rs/foo                                 # Scale a replicaset named 'foo' to 3
kubectl scale --replicas=3 -f foo.yaml                            # Scale a resource specified in "foo.yaml" to 3
kubectl scale --current-replicas=2 --replicas=3 deployment/mysql  # If the deployment named mysql's current size is 2, scale mysql to 3
kubectl scale --replicas=5 rc/foo rc/bar rc/baz                   # Scale multiple replication controllers


#Deleting resources 
==================================================================
kubectl delete -f ./pod.json                                      # Delete a pod using the type and name specified in pod.json
kubectl delete pod unwanted --now                                 # Delete a pod with no grace period
kubectl delete pod,service baz foo                                # Delete pods and services with same names "baz" and "foo"
kubectl delete pods,services -l name=myLabel                      # Delete pods and services with label name=myLabel
kubectl -n my-ns delete pod,svc --all                             # Delete all pods and services in namespace my-ns,
# Delete all pods matching the awk pattern1 or pattern2
kubectl get pods  -n mynamespace --no-headers=true | awk '/pattern1|pattern2/{print $1}' | xargs  kubectl delete -n mynamespace pod
kubectl delete ns dev

#Interacting with running Pods
==============================================================
kubectl logs my-pod                                 # dump pod logs (stdout)
kubectl logs -l name=myLabel                        # dump pod logs, with label name=myLabel (stdout)
kubectl logs my-pod --previous                      # dump pod logs (stdout) for a previous instantiation of a container
kubectl logs my-pod -c my-container                 # dump pod container logs (stdout, multi-container case)
kubectl logs -l name=myLabel -c my-container        # dump pod logs, with label name=myLabel (stdout)
kubectl logs my-pod -c my-container --previous      # dump pod container logs (stdout, multi-container case) for a previous instantiation of a container
kubectl logs -f my-pod                              # stream pod logs (stdout)
kubectl logs -f my-pod -c my-container              # stream pod container logs (stdout, multi-container case)
kubectl logs -f -l name=myLabel --all-containers    # stream all pods logs with label name=myLabel (stdout)
kubectl run -i --tty busybox --image=busybox:1.28 -- sh  # Run pod as interactive shell
kubectl run nginx --image=nginx -n mynamespace      # Start a single instance of nginx pod in the namespace of mynamespace
kubectl run nginx --image=nginx --dry-run=client -o yaml > pod.yaml
                                                    # Generate spec for running pod nginx and write it into a file called pod.yaml
kubectl attach my-pod -i                            # Attach to Running Container
kubectl port-forward my-pod 5000:6000               # Listen on port 5000 on the local machine and forward to port 6000 on my-pod

kubectl exec my-pod -- ls /                         # Run command in existing pod (1 container case) it will list the folders and files
kubectl exec --stdin --tty my-pod -- /bin/sh        # Interactive shell access to a running pod (1 container case), both commands same 

kubectl exec my-pod -c my-container -- ls /         # Run command in existing pod (multi-container case)
kubectl top pod POD_NAME --containers               # Show metrics for a given pod and its containers
kubectl top pod POD_NAME --sort-by=cpu              # Show metrics for a given pod and sort it by 'cpu' or 'memory'

kubectl exec -it pod/nginx-deployment-7fb96c846b-22j7j /bin/bash # go inside running  container and edit the container files, this command is very usefull

=========================================================================
#list the all lables

#list the pods 
$kubectl get pods
#list the pods with matching lables
$kubectl get pods -l app=<label-name>
#or when we want to display with multiple labels
$kubectl get pods -l environment=production,tier=frontend
#or using set based
$kubectl get pods -l 'environment in (production),tier in (frontend)'

#describe nodes with lables 
$kubectl describe nods | grep -i labels -A5

#Copying files and directories to and from containers
======================================================================
kubectl cp /tmp/foo_dir my-pod:/tmp/bar_dir            # Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the current namespace
kubectl cp /tmp/foo my-pod:/tmp/bar -c my-container    # Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container
kubectl cp /tmp/foo my-namespace/my-pod:/tmp/bar       # Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace my-namespace
kubectl cp my-namespace/my-pod:/tmp/foo /tmp/bar       # Copy /tmp/foo from a remote pod to /tmp/bar locally


Note: kubectl cp requires that the 'tar' binary is present in your container image. If 'tar' is not present, kubectl cp will fail. For advanced use cases, such as symlinks, wildcard expansion or file mode preservation consider using kubectl exec.

tar cf - /tmp/foo | kubectl exec -i -n my-namespace my-pod -- tar xf - -C /tmp/bar           # Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace my-namespace
kubectl exec -n my-namespace my-pod -- tar cf - /tmp/foo | tar xf - -C /tmp/bar    # Copy /tmp/foo from a remote pod to /tmp/bar locally


#Interacting with Deployments and Services 
=====================================================================================
kubectl logs deploy/my-deployment                         # dump Pod logs for a Deployment (single-container case)
kubectl logs deploy/my-deployment -c my-container         # dump Pod logs for a Deployment (multi-container case)

kubectl port-forward svc/my-service 5000                  # listen on local port 5000 and forward to port 5000 on Service backend
kubectl port-forward svc/my-service 5000:my-service-port  # listen on local port 5000 and forward to Service target port with name <my-service-port>

kubectl port-forward deploy/my-deployment 5000:6000       # listen on local port 5000 and forward to port 6000 on a Pod created by <my-deployment>
kubectl exec deploy/my-deployment -- ls                   # run command in first Pod and first container in Deployment (single- or multi-container cases)


#Interacting with Nodes and cluster
==============================================================================
kubectl cordon my-node                                                # Mark my-node as unschedulable
kubectl drain my-node                                                 # Drain my-node in preparation for maintenance
kubectl uncordon my-node                                              # Mark my-node as schedulable
kubectl top node my-node                                              # Show metrics for a given node
kubectl cluster-info                                                  # Display addresses of the master and services
kubectl cluster-info dump                                             # Dump current cluster state to stdout
kubectl cluster-info dump --output-directory=/path/to/cluster-state   # Dump current cluster state to /path/to/cluster-state

# View existing taints on which exist on current nodes.
kubectl get nodes -o='custom-columns=NodeName:.metadata.name,TaintKey:.spec.taints[*].key,TaintValue:.spec.taints[*].value,TaintEffect:.spec.taints[*].effect'

# If a taint with that key and effect already exists, its value is replaced as specified.
kubectl taint nodes foo dedicated=special-user:NoSchedule

#About configmap or create configmap
$ kubectl create configmap <name>
#list the configmap 
$ kubectl get configmap  
#describe the configmap
$ kubectl get configmaps <name> -o yaml
#output
data:
  special.how: very
  special.type: charm
kind: ConfigMap
metadata:
  creationTimestamp: "2023-06-12T11:44:11Z"
  name: special-config
  namespace: default
  resourceVersion: "961511"
  uid: 2a317cc2-57c9-4005-a3a8-dec8036287b0

#Create ConfigMaps from files 
$ kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties

#Create ConfigMaps from literal values 
You can use kubectl create configmap with the --from-literal argument to define a literal value from the command line:
$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm

#Delete configmap
kubectl delete configmap special-config
kubectl delete configmap env-config
kubectl delete configmap -l 'game-config in (config-4,config-5)'

#Delete worker nodes
=========================
List the nodes and get the <node-name> you want to drain or (remove from cluster)

$kubectl get nodes
1) First drain the node
$ kubectl drain <node-name>

You might have to ignore daemonsets and local-data in the machine
$kubectl drain <node-name> --ignore-daemonsets  --delete-emptydir-data

2) Edit instance group for nodes (Only if you are using kops)
$kops edit ig nodes

Set the MIN and MAX size to whatever it is -1 Just save the file (nothing extra to be done)

You still might see some pods in the drained node that are related to daemonsets like networking plugin, fluentd for logs, kubedns/coredns etc

3) Finally delete the node
$kubectl delete node <node-name>

4) Commit the state for KOPS in s3: (Only if you are using kops)
$kops update cluster --yes

OR (if you are using kubeadm)

If you are using kubeadm and would like to reset the machine to a state which was there before running kubeadm join then run
$kubeadm reset

Note: You need to execute kops update cluster --yes in order to commit the changes

================
#Remove worker node from Kubernetes

kubectl get nodes
kubectl drain < node-name > --ignore-daemonsets
kubectl delete node < node-name >
======================================

https://stackoverflow.com/questions/35757620/how-to-gracefully-remove-a-node-from-kubernetes

https://fabianlee.org/2022/03/08/kubernetes-deleting-a-gke-node-from-a-managed-instance-node-pool/#:~:text=If%20you%20need%20to%20delete,instance%20group%20for%20each%20region.

https://monowar-mukul.medium.com/kubernetes-remove-worker-node-from-the-cluster-and-completely-uninstall-af41e00c1244


=============================================================================================================================================
#GKE Commmands
============================================================================================================================================
#View node pools in a cluster
To list all the node pools of a cluster, run the gcloud container node-pools list command:
$ gcloud container node-pools list --cluster CLUSTER_NAME --region REGION_CODE
	NAME: default-pool
	MACHINE_TYPE: g1-small
	DISK_SIZE_GB: 50
	NODE_VERSION: 1.25.8-gke.500
To view details about a specific node pool, run the gcloud container node-pools describe command:
$ gcloud container node-pools describe POOL_NAME \
    --cluster CLUSTER_NAME \
	--region REGION_CODE
	gcloud container node-pools describe default-pool --cluster demo_gke --region us-central1

#Add a node pool
You can add a new node pool to a GKE Standard cluster using the gcloud CLI or the Google Cloud console. GKE also supports node auto-provisioning, which automatically manages the node pools in your cluster based on scaling requirements.

As a best practice in both cases, we recommend that you create and use a minimally-privileged Identity and Access Management (IAM) service account for your node pools to use instead of the Compute Engine default service account. For instructions to create a minimally-privileged service account, refer to Hardening your cluster's security'.

$gcloud container node-pools create POOL_NAME \
    --cluster CLUSTER_NAME \
    --service-account SERVICE_ACCOUNT
	
#Delete a node pool
gcloud container node-pools delete POOL_NAME \
    --cluster CLUSTER_NAME
	
#Resize a node pool
To resize a cluster's node pools, run the gcloud container clusters resize command':
$gcloud container clusters resize CLUSTER_NAME \
    --node-pool POOL_NAME \
    --num-nodes NUM_NODES	
$ gcloud container clusters resize demo-gke --node-pool default-pool   --num-nodes 2 --region us-central1

# you can get all the information about the  GKE cluster using the following command.
-------------------------------------------------------------------------------------
$ gcloud container clusters describe  demo-gke --region=us-central1

#Add a label to a node 
$kubectl get nodes --show-labels
$kubectl label nodes <your-node-name> disktype=ssd

#Manually upgrade a node pool
You can manually upgrade a node pool version to match the version of the control plane or to a previous version that is still available and is compatible with the control plane. The Kubernetes version and version skew support policy guarantees that control planes are compatible with nodes up to two minor versions older than the control plane. For example, Kubernetes 1.23 control planes are compatible with Kubernetes 1.21 nodes.

When you manually upgrade a node pool, GKE removes any labels you added to individual nodes using kubectl. To avoid this, apply labels to node pools instead.
$ gcloud container clusters upgrade CLUSTER_NAME \
  --node-pool=NODE_POOL_NAME
  
To specify a different version of GKE on nodes, use the optional --cluster-version flag:

$gcloud container clusters upgrade CLUSTER_NAME \
  --node-pool=NODE_POOL_NAME \
  --cluster-version VERSION
  
#Create a pod that gets scheduled to specific node 
#Approach1: Using nodeName
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: foo-node # schedule pod to specific node
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
	
#Deploy a Pod to a specific node pool Using nodeSelector
#Approach2: Using nodeSelector : First edit the ode and add the labels if we want too
You can explicitly deploy a Pod to a specific node pool by using a nodeSelector in your Pod manifest. nodeSelector schedules Pods onto nodes with a matching label.
All GKE node pools have labels with the following format: cloud.google.com/gke-nodepool: POOL_NAME. Add this label to the nodeSelector field in your Pod as shown in the following example:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    cloud.google.com/gke-nodepool: POOL_NAME
	
Also, you check the google kubernetes engine dashboard to view all the details about the cluster.

#Namespace:
namespaces provides a mechanism for isolating groups of resources within a single cluster.
Names of resources need to be unique within a namespace, but not across namespaces.

#Initial namespaces
Kubernetes starts with four initial namespaces:

default
Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.

#Viewing namespaces 
You can list the current namespaces in a cluster using:
$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   1d
kube-node-lease   Active   1d
kube-public       Active   1d
kube-system       Active   1d

#Create namespace
$kubectl create ns dev
#Delete namespace
$Kubetl delete ns dev

#creae pod in the dev namaspace
$kubectl run pod <pod-name> -n <namaspace-name> #this is by 
$kubectl apply -f <name>.yaml -n <namaspace-name> #declarative  approach
#Defining namespace in yaml file
#Create a pod that gets scheduled to specific node and assigned specific namespace and lables
#Approach1: Using nodeName
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels: 
    app: nginx
	end: dev
  namespace: dev
spec:
  nodeName: foo-node # schedule pod to specific node
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
