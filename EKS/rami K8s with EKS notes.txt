#!/bin/bash

#Official website
https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html

#Refer the below git repo for setup with amazon eks
https://github.com/vikas99341/k8s-masterclass/blob/main/EKS/eksctl

#Prerequisites
1> AWSCLI 
2> Kubectl
3> Eksctl --> eksctl version
4> Create an IAM role and attach to EC2 machine
   (Administrative, IAMFullAccess, CloudFormation, )

Kubernates
------------------------------------------------
#What is Kubernates?
Kubernates is also know as K8s, and it is open source container orchestration system for automatic software deployment, scaling and management of containerized applications.

#What is the kubectl?
Kubernetes provides a command line tool for communicating with a Kubernetes cluster's control plane, using the Kubernetes API. This tool is named kubectl.'
For configuration, kubectl looks for a file named config in the $HOME/.kube directory.
Using Kubectl allows you to create, inspect, update, and delete Kubernetes objects

#What is the eksctl?
eksctl is also command line interface tool for creating and managing clusters on EKS.

#Install AWS CLI
$curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
$unzip awscliv2.zip
$sudo ./aws/install
check AWS CLI Version
$aws --version


#kubernates setup in EKS
==============================================================================
https://github.com/vikas99341/k8s-masterclass/blob/main/EKS/eksctl
#Before starting this tutorial, you must install and configure the following tools and resources that you need to create and manage an Amazon EKS cluster.
kubectl and eksctl both must be required.
#kubectl : Refer official website https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html
=================================
#Determine whether you already have kubectl installed on your device.
$ kubectl version --short --client
#Install or update kubectl on Linux operating systems.   
$ curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.27.1/2023-04-19/bin/linux/amd64/kubectl
#Apply execute permissions to the binary.
$ chmod +x ./kubectl
#Copy the binary to a folder in your PATH. If you have already installed a version of kubectl, then we recommend creating a $HOME/bin/kubectl and ensuring that $HOME/bin comes first in your $PATH
$ mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$HOME/bin:$PATH
#(Optional) Add the $HOME/bin path to your shell initialization file so that it is configured when you open a shell.
$ echo 'export PATH=$HOME/bin:$PATH' >> ~/.bashrc
#After you install kubectl, you can verify its version.
$ kubectl version --short --client

#eksctl : Refer official website 
================================
https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html
https://github.com/weaveworks/eksctl/blob/main/README.md#installation  # For eksctl installation

#eksctl: is a simple command line tool for creating and managing Kubernetes clusters on Amazon EKS. eksctl provides the fastest and easiest way to create a new cluster with nodes for Amazon EKS. 

#Determine whether you already have eksctl installed on your device.
$ eksctl version
# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
#Approach1
----------
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH

curl -sLO "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
# (Optional) Verify checksum
curl -sL "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
sudo mv /tmp/eksctl /usr/local/bin

#Approach2:
---------
   ```sh
   curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
   sudo mv /tmp/eksctl /usr/local/bin
   eksctl version```

#Create IAM roles and attach to EC2 instance on which machine cluster runs
============================================================================
`Note: create IAM user with programmatic access if your bootstrap system is outside of AWS`
Refer https://eksctl.io/usage/minimum-iam-policies/

First create Roles :
IAMFULLAccess
EC2FullAccess 
CloudFormationFullAccess
AmazonEKSClusterPolicy.
AWSServiceRoleForAmazonEKS 
AdministratorAccess

and go go to EC2 instance and click on Actions --> Security ---> Modify IAM Role--> chose Role.

#Step1: Create your Amazon EKS cluster and nodes
==========================================
this topic includes steps to create a cluster and nodes with default settings. Before creating a cluster and nodes for production use, we recommend that you familiarize yourself with all settings and deploy a cluster and nodes with the settings that meet your requirements.
#Cluster:
========
https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html
#Nodes: ple refer this official website https://docs.aws.amazon.com/eks/latest/userguide/eks-compute.html
A Kubernetes node is a machine that runs containerized applications. Each node has the following components:
Container runtime – Software that's responsible for running the containers'
kubelet – Makes sure that containers are healthy and running within their associated Pod.
kube-proxy – Maintains network rules that allow communication to your Pods.

You can create a cluster with one of the following node types. To learn more about each type, see Amazon EKS nodes. After your cluster is deployed, you can add other node types.

#Fargate – Linux –:
Select this type of node if you want to run Linux applications on AWS Fargate. Fargate is a serverless compute engine that lets you deploy Kubernetes Pods without managing Amazon EC2 instances.

#Managed nodes – Linux :
Select this type of node if you want to run Amazon Linux applications on Amazon EC2 instances. Though not covered in this guide, you can also add Windows self-managed and Bottlerocket nodes to your cluster.

Create your Amazon EKS cluster with the following command. You can replace my-cluster with your own value.
$eksctl create cluster --name my-cluster --region region-code --fargate
$eksctl create cluster --name my-cluster --region ap-south-1 --fargate
 Ex:
   $eksctl create cluster --name cluster-name  \
   --region region-name \
   --node-type instance-type \
   --nodes-min 2 \
   --nodes-max 2 \ 
   --zones <AZ-1>,<AZ-2>
   $eksctl create cluster --name demo-cluster --region us-east-1 --node-type t2.medium
   $eksctl create cluster --name demo-cluster --region ap-south-1 --node-type t2.medium
#Output:
[root@k8s-mgmt-svr ~]# eksctl create cluster --name demo-cluster --region ap-south-1 --node-type t2.medium
2023-06-02 13:07:56 [ℹ]  eksctl version 0.143.0
2023-06-02 13:07:56 [ℹ]  using region ap-south-1
2023-06-02 13:07:56 [ℹ]  skipping ap-south-1c from selection because it doesn't support the following instance type(s): t2.medium
2023-06-02 13:07:56 [ℹ]  setting availability zones to [ap-south-1a ap-south-1b]
2023-06-02 13:07:56 [ℹ]  subnets for ap-south-1a - public:192.168.0.0/19 private:192.168.64.0/19
2023-06-02 13:07:56 [ℹ]  subnets for ap-south-1b - public:192.168.32.0/19 private:192.168.96.0/19
2023-06-02 13:07:56 [ℹ]  nodegroup "ng-dc7893ee" will use "" [AmazonLinux2/1.25]
2023-06-02 13:07:56 [ℹ]  using Kubernetes version 1.25
2023-06-02 13:07:56 [ℹ]  creating EKS cluster "demo-cluster" in "ap-south-1" region with managed nodes
2023-06-02 13:07:56 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2023-06-02 13:07:56 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=ap-south-1 --cluster=demo-cluster'
2023-06-02 13:07:56 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "demo-cluster" in "ap-south-1"
2023-06-02 13:07:56 [ℹ]  CloudWatch logging will not be enabled for cluster "demo-cluster" in "ap-south-1"
2023-06-02 13:07:56 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=ap-south-1 --cluster=demo-cluster'
2023-06-02 13:07:56 [ℹ]
2 sequential tasks: { create cluster control plane "demo-cluster",
    2 sequential sub-tasks: {
        wait for control plane to become ready,
        create managed nodegroup "ng-dc7893ee",
    }
}
2023-06-02 13:07:56 [ℹ]  building cluster stack "eksctl-demo-cluster-cluster"
2023-06-02 13:07:57 [ℹ]  deploying stack "eksctl-demo-cluster-cluster"
2023-06-02 13:08:27 [ℹ]  waiting for CloudFormation stack "eksctl-demo-cluster-cluster"
2023-06-02 13:23:06 [ℹ]  waiting for CloudFormation stack "eksctl-demo-cluster-nodegroup-ng-dc7893ee"
2023-06-02 13:23:06 [ℹ]  waiting for the control plane to become ready
2023-06-02 13:23:07 [✔]  saved kubeconfig as "/root/.kube/config"
2023-06-02 13:23:07 [ℹ]  no tasks
2023-06-02 13:23:07 [✔]  all EKS cluster resources for "demo-cluster" have been created
2023-06-02 13:23:07 [ℹ]  nodegroup "ng-dc7893ee" has 2 node(s)
2023-06-02 13:23:07 [ℹ]  node "ip-192-168-29-206.ap-south-1.compute.internal" is ready
2023-06-02 13:23:07 [ℹ]  node "ip-192-168-62-169.ap-south-1.compute.internal" is ready
2023-06-02 13:23:07 [ℹ]  waiting for at least 2 node(s) to become ready in "ng-dc7893ee"
2023-06-02 13:23:07 [ℹ]  nodegroup "ng-dc7893ee" has 2 node(s)
2023-06-02 13:23:07 [ℹ]  node "ip-192-168-29-206.ap-south-1.compute.internal" is ready
2023-06-02 13:23:07 [ℹ]  node "ip-192-168-62-169.ap-south-1.compute.internal" is ready
2023-06-02 13:23:09 [ℹ]  kubectl command should work with "/root/.kube/config", try 'kubectl get nodes'
2023-06-02 13:23:09 [✔]  EKS cluster "demo-cluster" in "ap-south-1" region is ready
[root@k8s-mgmt-svr ~]#'

   
Note: Cluster creation takes several minutes. During creation you will see several lines of output. The last line of output is similar to the following example line.
#Note:
Once you have created a cluster, you will find that cluster credentials and about the cluster in this location ~/.kube/config
$ cat ~/.kube/config 

eksctl created a kubectl config file in ~/.kube or added the new cluster's configuration within an existing config file in ~/.kube on your computer.'

After cluster creation is complete, view the AWS CloudFormation stack named demo-cluster-cluster in the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation to see all of the resources that were created.

#Step 2: View Kubernetes resources
=================================================================
#View your cluster nodes.
$kubectl get nodes -o wide

#For Example output like this
NAME                                                STATUS   ROLES    AGE     VERSION              INTERNAL-IP   EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
fargate-ip-192-0-2-0.region-code.compute.internal   Ready    <none>   8m3s    v1.2.3-eks-1234567   192.0.2.0     <none>        Amazon Linux 2   1.23.456-789.012.amzn2.x86_64   containerd://1.2.3
fargate-ip-192-0-2-1.region-code.compute.internal   Ready    <none>   7m30s   v1.2.3-eks-1234567   192-0-2-1     <none>        Amazon Linux 2   1.23.456-789.012.amzn2.x86_64   containerd://1.2.3

#two ways to create pods in k8s
1> using command line tool like below
2> using ml file 

#Approach1
#create an pod in kubernets cluser using kubectl command?
$ kubectl run nginx --image=nginx --restart=Never  # This will create a pod named nginx, running with the nginx image on Docker Hub. And by setting the flag --restart=Never we tell Kubernetes to create a single pod rather than a Deployment.

#Approach2 using yml file
Step1: go to master node or control plain node
Step2: create an directory with name pods
Step3: cd pods 
Step4: create an file and named to pod1.yaml file
	apiVersion: v1
	kind: Pod        #kind: pod represents this is the pod  # kind: Deployment represent deployment in pod
	metadata:
	  name: webserver
	spec:
	  containers:
	  - name: webserver1       #container1
		image: nginx:latest
		ports:
		- containerPort: 80
	  - name: webserver2      #container1  
		image: httpd
#Note: We can have N number of containers in pod.
Step5: run the yml file using below command
$kubectl create -f pod1.yaml

#Note: We can not control the pod that will create in which node. it get create based on the scheduler algorithm by default.
#we can control the pod that need to be create in specific nodes using node affective
When we run the  above pod creation command then kubernets automatically generates yml file, if we wanna see the full yml file issue the following commmad
$kubectl get pod/nameofthepod yml | less

#View the workloads running on your cluster or 
#to know the pods on which worker node running
$kubectl get pods -A -o wide

#for example output is as follows
NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE   IP          NODE                                                NOMINATED NODE   READINESS GATES
kube-system   coredns-1234567890-abcde   1/1     Running   0          18m   192.0.2.0   fargate-ip-192-0-2-0.region-code.compute.internal   <none>           <none>
kube-system   coredns-1234567890-12345   1/1     Running   0          18m   192.0.2.1   fargate-ip-192-0-2-1.region-code.compute.internal   <none>           <none>

#List the pods
$kubectl get pods

#Describe the pods
$kubectl describe pod <name of the pod>  or $kubectl describe pod/<name-of-the-pod>
$kubectl describe pod coredns-1234567890-abcde

#to delete the pod
$kubectl delete name-of-the-pod

#To delete the EKS cluster
$ eksctl delete cluster demo-cluster --region ap-south-1

# Deploying Nginx pods on Kubernetes
====================================
#Deploying Nginx Container:
$ kubectl create deployment  demo-nginx --image=nginx --replicas=2 --port=80    #here replicas represent no of pods need to be run

$kubectl get deployment
[root@k8s-mgmt-svr ~]# kubectl get deployment
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
demo-nginx   2/2     2            2           22m

$ kubectl get all
[root@k8s-mgmt-svr .kube]# kubectl get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   45m

#Add another worker nodes to cluster using eksctl
Firtst run the below command in master node and it givee you the join command that command run on the worker nodes.
$kubectl 

#Add another worker nodes to cluster using kubeadm
Still on the existing master node, print the join command
$kubeadm token create --print-join-command  #  run this command on the master node only
#You will get an output like this and run the below output commmad in worker node only.
kubeadm join k8s-endpoint:6443 --token 4iegnp.x2tfqdd9gl93zz1o --discovery-token-ca-cert-hash sha256:
worker-node1> $ kubeadm join k8s-endpoint:6443 --token 4iegnp.x2tfqdd9gl93zz1o --discovery-token-ca-cert-hash sha256:

#Change the master-node ip address
$kubectl -n kube-system edit svc fci-proxy-router
Go to the first occurrence of two sections that list externalIPs. For example, suppose that 172.16.241.36 was entered for both the manager.ip and the manager.external_ip options:
externalIPs:
- 172.16.241.36
- 172.16.241.36

#Expose the deployment as service. This will create an ELB in front of those 2 containers and allow us to publicly access them.
$ kubectl expose deployment demo-nginx --port=80 --type=LoadBalancer

$ kubectl get svc demo-nginx # this commands will give you the url to access remotely.
[root@k8s-mgmt-svr ~]# kubectl get svc demo-nginx
NAME         TYPE           CLUSTER-IP       EXTERNAL-IP                                                                PORT(S)        AGE
demo-nginx   LoadBalancer   10.100.143.146   a8f787949ee6f4f0e8cddfcf68db9d2d-1322773865.ap-south-1.elb.amazonaws.com   80:30724/TCP   31m

# go to browser and hit with this url then u will see the nginx default index.html page content
a8f787949ee6f4f0e8cddfcf68db9d2d-1322773865.ap-south-1.elb.amazonaws.com

#What is the kubelet in kubernets?
kubelet is the primary "node agent" that runs on each node. It can register the nore with api server using one of it one of the hostname.
The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod.
	
1> Where were you run the eksctl cluster commmand
2> what is different between kubectl and eksctl
3> Which parameter is should use to scale up and scale down? CPU Utalization by default > 70 %

4> u said u will teach about service for expose service
